{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d86d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "934b764b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ed9c': 'green', '0001': 'blue', '3d57': 'red'}\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import configparser\n",
    "config = configparser.ConfigParser()\n",
    "config.read('agg-config.txt')\n",
    "\n",
    "\n",
    "time_step = int(config.get('Configuration', 'time_step'))\n",
    "time_step2 = int(config.get('Configuration', 'time_step2'))\n",
    "\n",
    "window_size = int(config.get('Configuration', 'window_size'))\n",
    "processed_data_folder = config.get('Configuration', 'processed_data_folder')\n",
    "raw_data_folder = config.get('Configuration', 'raw_data_folder')\n",
    "\n",
    "ini_date = config.get('Configuration', 'ini_date')\n",
    "end_date = config.get('Configuration', 'end_date')\n",
    "\n",
    "users=config.get('Configuration', 'users')\n",
    "users = [(item.strip()) for item in users.split(',')]\n",
    "\n",
    "activities=config.get('Configuration', 'learn_activities')\n",
    "activities = [(item.strip()) for item in activities.split(',')]\n",
    "\n",
    "cross_days=config.get('Configuration', 'cross_days')\n",
    "cross_days = [(item.strip()) for item in cross_days.split(',')]\n",
    "\n",
    "sensors=config.get('Configuration', 'sensors')\n",
    "sensors = [(item.strip()) for item in sensors.split(',')]\n",
    "\n",
    "w = int(config.get('Configuration', 'w'))\n",
    "h = int(config.get('Configuration', 'h'))\n",
    "\n",
    "user_color={}\n",
    "for user in users:\n",
    "    user_color[user]=config.get('Colors', user)\n",
    "print(user_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44ea50ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-07 02:00:00 Init date is 1720310400 day: 19911 2024-07-07 02:00:00\n",
      "2024-07-08 01:59:59 End date is 1720396799 day: 19912 2024-07-08 01:59:59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(19911, 1720310400, 1720396799)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pandas import read_csv    \n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "off_zone=60*60*2\n",
    "\n",
    "def day_time(t0,tN,ti):\n",
    "    return int((int)((ti+off_zone)/(60*60*24)))\n",
    "\n",
    "\n",
    "def time2str(t0,tN,tt):\n",
    "    return datetime.fromtimestamp(tt).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "\n",
    "def day_time0(t0,tN,ti):\n",
    "    return day_time(ti)-day_time(t0)+1\n",
    "\n",
    "def relT(t0,tN,ti):\n",
    "    return (int)((ti-t0)/time_step2)\n",
    "\n",
    "def sizeT(t0,tN):\n",
    "    return relT(t0,tN,tN)+1\n",
    "\n",
    "\n",
    "def initTime(ini_date,end_date):\n",
    "    tN = (int)(datetime.strptime(end_date, '%Y-%m-%d %H:%M:%S').timestamp())\n",
    "\n",
    "    t0 = (int)(datetime.strptime(ini_date, '%Y-%m-%d %H:%M:%S').timestamp())\n",
    "    print (ini_date,\"Init date is\", t0, \"day:\",day_time(t0,tN,t0), time2str(t0,tN,t0))\n",
    "    print (end_date,\"End date is\", tN, \"day:\",day_time(t0,tN,tN), time2str(t0,tN,tN))\n",
    "\n",
    "    day=list(range(day_time(t0,tN,t0),day_time(t0,tN,tN)+1))[0]\n",
    "    return day,t0,tN\n",
    "    \n",
    "initTime(ini_date,end_date)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daeeab4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbe748a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4ed8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8787ce8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-07-05\n",
      "New ini_date: 2024-07-05 02:00:00\n",
      "New end_date: 2024-07-06 01:59:59\n",
      "2024-07-05 02:00:00 Init date is 1720137600 day: 19909 2024-07-05 02:00:00\n",
      "2024-07-06 01:59:59 End date is 1720223999 day: 19910 2024-07-06 01:59:59\n",
      "2024-07-06\n",
      "New ini_date: 2024-07-06 02:00:00\n",
      "New end_date: 2024-07-07 01:59:59\n",
      "2024-07-06 02:00:00 Init date is 1720224000 day: 19910 2024-07-06 02:00:00\n",
      "2024-07-07 01:59:59 End date is 1720310399 day: 19911 2024-07-07 01:59:59\n",
      "2024-07-07\n",
      "New ini_date: 2024-07-07 02:00:00\n",
      "New end_date: 2024-07-08 01:59:59\n",
      "2024-07-07 02:00:00 Init date is 1720310400 day: 19911 2024-07-07 02:00:00\n",
      "2024-07-08 01:59:59 End date is 1720396799 day: 19912 2024-07-08 01:59:59\n",
      "[19909, 19910, 19911]\n",
      "{19909: (1720137600, 1720223999), 19910: (1720224000, 1720310399), 19911: (1720310400, 1720396799)}\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "days=[]\n",
    "t0N_days={}\n",
    "\n",
    "for cross_day in cross_days:\n",
    "    print(cross_day)\n",
    "    # Convertir las fechas a objetos datetime\n",
    "    ini_date_dt = datetime.strptime(ini_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d %H:%M:%S\")\n",
    "    cross_day_dt = datetime.strptime(cross_day, \"%Y-%m-%d\")\n",
    "\n",
    "    # Reemplazar el día manteniendo la hora original\n",
    "    new_ini_date = cross_day_dt.replace(hour=ini_date_dt.hour, minute=ini_date_dt.minute, second=ini_date_dt.second)\n",
    "    new_end_date = cross_day_dt.replace(hour=end_date_dt.hour, minute=end_date_dt.minute, second=end_date_dt.second) + timedelta(days=1)\n",
    "\n",
    "    # Convertir las fechas de nuevo a cadena si es necesario\n",
    "    new_ini_date_str = new_ini_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    new_end_date_str = new_end_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    print(f\"New ini_date: {new_ini_date_str}\")\n",
    "    print(f\"New end_date: {new_end_date_str}\")\n",
    "    day,t0,tN=initTime(new_ini_date_str,new_end_date_str)\n",
    "    days.append(day)\n",
    "    t0N_days[day]=(t0,tN)\n",
    "    \n",
    "\n",
    "print(days)\n",
    "print(t0N_days)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "63f2ed21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t resting\n",
      "\t exit\n",
      "\t cooking\n",
      "\t toileting\n",
      "\t shower\n",
      "\t pc\n",
      "\t sleep\n",
      "\t kitchen\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "for ax,activity in enumerate(activities):\n",
    "    print(\"\\t\",activity)\n",
    "    for ux,user in enumerate(users):\n",
    "        for dx,day in enumerate(days):\n",
    "            series=read_csv(processed_data_folder+\"/DAY_\"+str(day)+\"/act/loc.\"+user+\".\"+activity+\".tsv\", sep=\"\\t\",parse_dates=True, header=None, names=[\"time\",\"loc\"])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9631e6b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 3, 1440)\n",
      "\t fridge_13\n",
      "./data-har//DAY_19909/object_sensor/fridge_13.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/fridge_13.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/fridge_13.tsv\n",
      "19911 (1440,)\n",
      "\t tap_22\n",
      "./data-har//DAY_19909/object_sensor/tap_22.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/tap_22.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/tap_22.tsv\n",
      "19911 (1440,)\n",
      "\t micro_5\n",
      "./data-har//DAY_19909/object_sensor/micro_5.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/micro_5.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/micro_5.tsv\n",
      "19911 (1440,)\n",
      "\t pans_8\n",
      "./data-har//DAY_19909/object_sensor/pans_8.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/pans_8.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/pans_8.tsv\n",
      "19911 (1440,)\n",
      "\t dishes_9\n",
      "./data-har//DAY_19909/object_sensor/dishes_9.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/dishes_9.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/dishes_9.tsv\n",
      "19911 (1440,)\n",
      "\t shower_24_apertura\n",
      "./data-har//DAY_19909/object_sensor/shower_24_apertura.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/shower_24_apertura.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/shower_24_apertura.tsv\n",
      "19911 (1440,)\n",
      "\t hum_shower_25\n",
      "./data-har//DAY_19909/object_sensor/hum_shower_25.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/hum_shower_25.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/hum_shower_25.tsv\n",
      "19911 (1440,)\n",
      "\t shower_31\n",
      "./data-har//DAY_19909/object_sensor/shower_31.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/shower_31.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/shower_31.tsv\n",
      "19911 (1440,)\n",
      "\t wc_17\n",
      "./data-har//DAY_19909/object_sensor/wc_17.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/wc_17.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/wc_17.tsv\n",
      "19911 (1440,)\n",
      "\t sink_21\n",
      "./data-har//DAY_19909/object_sensor/sink_21.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/sink_21.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/sink_21.tsv\n",
      "19911 (1440,)\n",
      "\t pc_1_current_consumption\n",
      "./data-har//DAY_19909/object_sensor/pc_1_current_consumption.tsv\n",
      "19909 (1440,)\n",
      "./data-har//DAY_19910/object_sensor/pc_1_current_consumption.tsv\n",
      "19910 (1440,)\n",
      "./data-har//DAY_19911/object_sensor/pc_1_current_consumption.tsv\n",
      "19911 (1440,)\n"
     ]
    }
   ],
   "source": [
    "act_sensors=np.zeros((len(sensors),len(days),sizeT(t0,tN)))\n",
    "print(act_sensors.shape)\n",
    "for sx,sensor in enumerate(sensors):\n",
    "    print(\"\\t\",sensor)\n",
    "    for dx,day in enumerate(days):\n",
    "        print(processed_data_folder+\"/DAY_\"+str(day)+\"/object_sensor/\"+sensor+\".tsv\")\n",
    "        series=read_csv(processed_data_folder+\"/DAY_\"+str(day)+\"/object_sensor/\"+sensor+\".tsv\", sep=\"\\t\",parse_dates=True, header=None, names=[\"time\",\"status\"])\n",
    "        series=series[\"status\"].values\n",
    "        print(day,series.shape)\n",
    "        act_sensors[sx][dx]=series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc04e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data-har//DAY_19909/act/INTERVAL.ed9c.resting.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.ed9c.resting.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.ed9c.resting.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.0001.resting.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.0001.resting.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.0001.resting.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.3d57.resting.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.3d57.resting.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.3d57.resting.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.ed9c.exit.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.ed9c.exit.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.ed9c.exit.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.0001.exit.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.0001.exit.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.0001.exit.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.3d57.exit.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.3d57.exit.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.3d57.exit.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.ed9c.cooking.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.ed9c.cooking.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.ed9c.cooking.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.0001.cooking.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.0001.cooking.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.0001.cooking.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.3d57.cooking.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.3d57.cooking.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.3d57.cooking.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.ed9c.toileting.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.ed9c.toileting.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.ed9c.toileting.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.0001.toileting.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.0001.toileting.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.0001.toileting.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.3d57.toileting.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.3d57.toileting.tsv\n",
      "./data-har//DAY_19911/act/INTERVAL.3d57.toileting.tsv\n",
      "./data-har//DAY_19909/act/INTERVAL.ed9c.shower.tsv\n",
      "./data-har//DAY_19910/act/INTERVAL.ed9c.shower.tsv\n"
     ]
    }
   ],
   "source": [
    "user_activity=np.zeros((len(activities),len(users),len(days),sizeT(t0,tN)))\n",
    "for ax,activity in enumerate(activities):\n",
    "    for ux,user in enumerate(users):\n",
    "        for dx,day in enumerate(days):\n",
    "            print(processed_data_folder+\"/DAY_\"+str(day)+\"/act/INTERVAL.\"+user+\".\"+activity+\".tsv\")\n",
    "            df_total=read_csv(processed_data_folder+\"/DAY_\"+str(day)+\"/act/INTERVAL.\"+user+\".\"+activity+\".tsv\", sep=\"\\t\",parse_dates=True, header=None, names=[\"ix\",\"d0\",\"dN\"])\n",
    "            #print(df_total)\n",
    "            #print(pd.to_datetime(df_total['d0']))\n",
    "            df_total['d0'] = pd.to_datetime(df_total['d0'])\n",
    "            df_total['dN'] = pd.to_datetime(df_total['dN'])\n",
    "            #print(df_total)\n",
    "\n",
    "            (t0,tN)=t0N_days[day]\n",
    "            ts=list(range(t0,tN,time_step2))\n",
    "            \n",
    "            for tx,tt in enumerate(ts):\n",
    "                ttt=pd.to_datetime(time2str(t0,tN,tt))\n",
    "#                print(ttt)\n",
    "                row = df_total[(df_total['d0'] <= ttt) & (df_total['dN'] >= ttt)]\n",
    "#                print(\"df_in#tervalo\",row)\n",
    "                if(row.size>0):\n",
    "                    user_activity[ax][ux][dx][tx]=1\n",
    "\n",
    "for ax,activity in enumerate(activities):\n",
    "    for ux,user in enumerate(users):\n",
    "        for dx,day in enumerate(days):\n",
    "            (t0,tN)=t0N_days[day]\n",
    "            ts=list(range(t0,tN,time_step2))\n",
    "            with open(processed_data_folder+\"/DAY_\"+str(day)+\"/act/LABEL3.\"+user+\".\"+activity+\".tsv\", \"w\") as file:\n",
    "                for tx,tt in enumerate(ts):\n",
    "                    file.write(time2str(t0,tN,tt)+\"\\t\"+str(user_activity[ax][ux][dx][tx])+\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b92a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, LSTM, Dense, Flatten, concatenate, Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras.layers import Lambda, Dot,Reshape,Multiply, Input, Conv1D, Conv2D, ConvLSTM2D,ReLU,UpSampling2D, BatchNormalization,\\\n",
    "                                    Add, AveragePooling2D, Flatten, Dense,Dropout,LSTM,TimeDistributed,MaxPooling1D, Concatenate\n",
    "\n",
    "\n",
    "N1=len(activities)\n",
    "N2=len(activities)\n",
    "N3=len(sensors)\n",
    "\n",
    "TWA=30\n",
    "TWB=15\n",
    "T=TWA+TWB\n",
    "\n",
    "def getModelHAR():\n",
    "    # Definición de las entradas\n",
    "    input1 = Input(shape=(N1, T, 1))\n",
    "    input2 = Input(shape=(N2, T, 1))\n",
    "    input3 = Input(shape=(N3, T, 1))\n",
    "\n",
    "    # Bloque para la primera secuencia\n",
    "    conv1_1 = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))(input1)\n",
    "    conv1_2 = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))(conv1_1)\n",
    "    flat1 = TimeDistributed(Flatten())(conv1_2)\n",
    "\n",
    "    # Bloque para la segunda secuencia\n",
    "    conv2_1 = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))(input2)\n",
    "    conv2_2 = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))(conv2_1)\n",
    "    flat2 = TimeDistributed(Flatten())(conv2_2)\n",
    "\n",
    "    # Bloque para la tercera secuencia\n",
    "    conv3_1 = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))(input3)\n",
    "    conv3_2 = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))(conv3_1)\n",
    "    flat3 = TimeDistributed(Flatten())(conv3_2)\n",
    "\n",
    "    # Concatenación de los tres bloques\n",
    "    merged = concatenate([flat1, flat2, flat3],axis=1)\n",
    "\n",
    "    # Primera capa LSTM después de la concatenación\n",
    "    lstm_merged1 = LSTM(256, return_sequences=True)(merged)\n",
    "\n",
    "    # Segunda capa LSTM después de la concatenación\n",
    "    lstm_merged2 = LSTM(512, return_sequences=False)(lstm_merged1)\n",
    "\n",
    "    # Capa densa para la clasificación binaria\n",
    "    dense = Dense(128, activation='relu')(lstm_merged2)\n",
    "    output = Dense(1, activation='sigmoid')(dense)\n",
    "\n",
    "    # Definición del modelo\n",
    "    model = Model(inputs=[input1, input2, input3], outputs=output)\n",
    "\n",
    "    # Compilación del modelo\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Instanciación del modelo\n",
    "model = getModelHAR()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "386cb41e-bb1e-4043-b002-ea2c550f5dbd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a216fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "\n",
    "SizeTrainOk=1000\n",
    "SizeTrainKo=1000\n",
    "\n",
    "\n",
    "for ax,activity in enumerate(activities):\n",
    "        print(ax,activity)\n",
    "        for dx,day in enumerate(days):\n",
    "            print(\"\\t\",dx,day)\n",
    "            other_dx= [other for other in range(0,len(days)) if other != dx]\n",
    "            \n",
    "            ##TRAING DATA\n",
    "            L1=[]\n",
    "            L2=[]\n",
    "            S=[]\n",
    "            Y=[]\n",
    "            for ux,user in enumerate(users):\n",
    "                    print(\"\\t\\t\",ux,user)\n",
    "                    other_ux= [other for other in range(0,len(users)) if other != ux]\n",
    "                    print(day,user,other_dx,other_ux)\n",
    "\n",
    "                    #OK act\n",
    "                    for ix in range(0,SizeTrainOk):\n",
    "                        #print(\"SizeTrainOk\",ix,\"vs\",SizeTrainOk)\n",
    "                        filtered_elements = user_activity[ax, ux, other_dx, :]\n",
    "\n",
    "                        # Encontramos los elementos que son 1\n",
    "                        indices = np.argwhere(filtered_elements == 1)\n",
    "\n",
    "                        if(len(indices)==0):\n",
    "                            #print(\"\\t\\t\\t\",\"Non activity for user \")\n",
    "                            break\n",
    "                        \n",
    "                        # Para obtener las posiciones relativas a las dimensiones originales\n",
    "                        indexs = [(ax, ux, other_dx[idx[0]], idx[1]) for idx in indices]\n",
    "                        index = random.choice(indexs)\n",
    "                        #print(\"\\t\\t\\t\",index,user_activity[index])\n",
    "                        \n",
    "                        (ia,iu,idy,tt)=index\n",
    "                        \n",
    "                        if(tt<TWA):\n",
    "                            tt=TWA\n",
    "                        if(tt>=sizeT(t0,tN)-TWB):\n",
    "                            tt=sizeT(t0,tN)-TWB\n",
    "                        \n",
    "                        \n",
    "                        X1s=[]\n",
    "                        X2s=[]\n",
    "                        for ax2,activity2 in enumerate(activities):\n",
    "                            X1s.append(loc_user[ax2][iu][idy][tt-TWA:tt+TWB])\n",
    "                            X2s.append(np.amax(loc_user[ax2,other_ux,idy,tt-TWA:tt+TWB],axis=0))\n",
    "                            \n",
    "                            \n",
    "                        Ss=[]\n",
    "                        for sx, sensor in enumerate(sensors):\n",
    "                            Ss.append(act_sensors[sx][idy][tt-TWA:tt+TWB])\n",
    "                        L1.append(X1s)\n",
    "                        L2.append(X2s)\n",
    "                        S.append(Ss)\n",
    "                        Y.append(user_activity[index])\n",
    "                        \n",
    "                    #KO act\n",
    "                    ix=0\n",
    "                    while ix < SizeTrainKo:\n",
    "                        #print(\"SizeTrainKo\",ix,\"vs\",SizeTrainKo)\n",
    "                        filtered_elements_specific = user_activity[ax, ux, other_dx, :]\n",
    "\n",
    "                        # Encontramos los elementos que son 1\n",
    "                        indices = np.argwhere(filtered_elements == 0)\n",
    "\n",
    "                        if(len(indices)==0):\n",
    "                            print(\"\\t\\t\\t\",\"Non activity for user \")\n",
    "                            break\n",
    "                        \n",
    "                        # Para obtener las posiciones relativas a las dimensiones originales\n",
    "                        indexs = [(ax, ux, other_dx[idx[0]], idx[1]) for idx in indices]\n",
    "                        index = random.choice(indexs)\n",
    "                        #print(\"\\t\\t\\t\",index,user_activity[index])\n",
    "                        \n",
    "                        (ia,iu,idy,tt)=index\n",
    "                        \n",
    "                        if(tt<TWA):\n",
    "                            tt=TWA\n",
    "                        if(tt>=sizeT(t0,tN)-TWB):\n",
    "                            tt=sizeT(t0,tN)-TWB\n",
    "                        \n",
    "                        \n",
    "                        X1s=[]\n",
    "                        X2s=[]\n",
    "                        none=False\n",
    "                        for ax2,activity2 in enumerate(activities):\n",
    "                            if(ax2==ax):\n",
    "                                r=np.random.rand()\n",
    "                                #print(\" removing loc?:\",loc_user[ax2][iu][idy][tt],r)\n",
    "                                if(r>loc_user[ax2][iu][idy][tt]+0.05):\n",
    "                                    #print(\"YES  removing loc:\",loc_user[ax2][iu][idy][tt])\n",
    "                                    none=True\n",
    "                                    break\n",
    "                            X1s.append(loc_user[ax2][iu][idy][tt-TWA:tt+TWB])\n",
    "                            X2s.append(np.amax(loc_user[ax2,other_ux,idy,tt-TWA:tt+TWB],axis=0))\n",
    "                            \n",
    "                            \n",
    "                        Ss=[]\n",
    "                        for sx, sensor in enumerate(sensors):\n",
    "                            Ss.append(act_sensors[sx][idy][tt-TWA:tt+TWB])\n",
    "                        if not none:\n",
    "                            L1.append(X1s)\n",
    "                            L2.append(X2s)\n",
    "                            S.append(Ss)\n",
    "                            Y.append(user_activity[index])       \n",
    "                            ix=ix+1\n",
    "\n",
    "            L1=np.array(L1)        \n",
    "            L2=np.array(L2)        \n",
    "            S=np.array(S)\n",
    "            Y=np.array(Y)\n",
    "            print(\"TRAIN\",activity,day,L1.shape,L2.shape,S.shape,Y.shape)\n",
    "            \n",
    "            \n",
    "            model=getModelHAR()\n",
    "            model.summary()\n",
    "            \n",
    "            ##TEST DATA\n",
    "            TL1=[]\n",
    "            TL2=[]\n",
    "            TS=[]\n",
    "            TY=[]\n",
    "            \n",
    "            \n",
    "            (t0,tN)=t0N_days[day]\n",
    "            ts=list(range(TWA,sizeT(t0,tN)-TWB))\n",
    "            \n",
    "            for ux2,user2 in enumerate(users):\n",
    "                other_ux2= [other for other in range(0,len(users)) if other != ux2]\n",
    "\n",
    "                for tx in ts:\n",
    "\n",
    "                    Ss=[]\n",
    "                    for sx, sensor in enumerate(sensors):\n",
    "                        Ss.append(act_sensors[sx][dx][tx-TWA:tx+TWB])\n",
    "                    \n",
    "                    X1s=[]\n",
    "                    X2s=[]\n",
    "                    for ax2,activity2 in enumerate(activities):\n",
    "                        X1s.append(loc_user[ax2][ux2][dx][tx-TWA:tx+TWB])\n",
    "                        X2s.append(np.amax(loc_user[ax2,other_ux2,dx,tx-TWA:tx+TWB],axis=0))\n",
    "\n",
    "\n",
    "                    TL1.append(X1s)\n",
    "                    TL2.append(X2s)\n",
    "                    TS.append(Ss)\n",
    "                    TY.append(user_activity[ax, ux2, dx, tx])\n",
    "                        \n",
    "            TL1=np.array(TL1)        \n",
    "            TL2=np.array(TL2)        \n",
    "            TS=np.array(TS)\n",
    "            TY=np.array(TY)\n",
    "            print(\"EVAL\",activity,day,TL1.shape,TL2.shape,TS.shape,TY.shape)\n",
    "            \n",
    "            history = model.fit([L1,L2,S] , Y, shuffle=True, batch_size=32,epochs=50, validation_data = ([TL1,TL2,TS], TY))\n",
    "            YT_prediction = model.predict([TL1,TL2,TS])\n",
    "            print(YT_prediction.shape)\n",
    "            \n",
    "            ix=0\n",
    "            for ux2,user2 in enumerate(users):\n",
    "                with open(processed_data_folder+\"/DAY_\"+str(day)+\"/act/PREDICTION.\"+user2+\".\"+activity+\".tsv\", \"w\") as file:\n",
    "                    for tx in (ts):\n",
    "                        tt=tx*time_step2+t0\n",
    "                        #print(\"\\t\"+time2str(t0,tN,tt)+\"\\t\"+str(TY[ix])+\"\\t\"+str(YT_prediction[ix][0]))\n",
    "                        file.write(\"\\t\"+time2str(t0,tN,tt)+\"\\t\"+str(TY[ix])+\"\\t\"+str(YT_prediction[ix][0])+\"\\n\")\n",
    "                        \n",
    "                        ix=ix+1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2fc952",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
